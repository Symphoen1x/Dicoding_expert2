# -*- coding: utf-8 -*-
"""Maaf salah file tadi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DC-RaV7XAKBAgHw5amVVcVVEvxvhMnIs

Nama: Isa Aulia Almadani <br>
Dom: Sukoharjo

# Manage file import from kaggle
"""

!pip install -q kaggle
from google.colab import files
files.upload()

!mkdir ~/.kaggle
!mv kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list -s ' rice dataset'

!kaggle datasets download -d 'muratkokludataset/rice-image-dataset    '

"""# Prepare Dataset

# Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import json
import zipfile


# from keras.layers import Input
from keras.applications import EfficientNetB7, MobileNetV2, EfficientNetV2M

import tensorflow as tf
from keras.layers import Flatten
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Conv2D, Dropout, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
import matplotlib.pyplot as plt
from warnings import filterwarnings
from sklearn.metrics import classification_report, confusion_matrix

# %matplotlib inline
import matplotlib.pyplot as plt

filezip = "rice-image-dataset    .zip"
extractZip = zipfile.ZipFile(filezip, 'r')
extractZip.extractall("datasets")

os.listdir("/content/datasets/Rice_Image_Dataset")

dataset_path = '/content/datasets/Rice_Image_Dataset'
class_samples = {}

# Loop melalui setiap kelas di dalam folder dataset
for class_folder in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_folder)

    if os.path.isdir(class_path):
        num_class = len([file for file in os.listdir(class_path) if file.endswith('.jpg')])

        class_samples[class_folder] = num_class

        # Loop melalui setiap file di dalam kelas
        for dirname, _, filenames in os.walk(class_path):
            for filename in filenames:
                file_path = os.path.join(dirname, filename)
                print(file_path)

for class_name, num_class in class_samples.items():
    print(f"Kelas '{class_name}': {num_class} images")

"""# Preprocessing"""

dataset_dir = "/content/datasets/Rice_Image_Dataset"

data_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode="nearest",
    validation_split=0.2
)
train_generator = data_gen.flow_from_directory(
    dataset_dir,
    target_size=(224, 224),
    batch_size=64,
    shuffle = True,
    color_mode = 'rgb',
    class_mode="categorical",
    subset="training"
)

validation_generator = data_gen.flow_from_directory(
    dataset_dir,
    target_size=(224, 224),
    batch_size=64,
    shuffle = True,
    color_mode = 'rgb',
    class_mode="categorical",
    subset="validation"
)

"""#Building model"""

pre_trained_model = MobileNetV2(weights="imagenet", include_top=False,
                                input_shape=(224, 224, 3))

pre_trained_model.trainable = False

model = Sequential([
    pre_trained_model,
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dropout(0.5),
    # Conv2D(64, (3, 3), activation='relu', padding='same'),
    # Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0,1),
    Dense(64, activation='relu'),
    Dropout(0.1),
    BatchNormalization(),
    Dense(5, activation='softmax')
])

learning_rate = 0.001
optimizer = 'adam'
model.compile(
    loss='categorical_crossentropy',
    optimizer=optimizer,
    metrics=['accuracy'],
)
model.summary()

# checkpoint = ModelCheckpoint(
#     "best_model.h5",
#     monitor="val_accuracy",
#     save_best_only=True,
#     mode="max",
#     verbose=2
# )
reduce_lr = ReduceLROnPlateau(monitor= 'val_accuracy', factor=0.3, patience=2, min_delta=0.001)

optimizer = 'adam'
model.compile(
    loss='categorical_crossentropy',
    optimizer=optimizer,
    metrics=['accuracy'],
)

class stopCallBack(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if self.has_reached_accuracy(logs):
            print(' Stop training model, acc & val_acc > 92% ')
            self.model.stop_training = True

    def has_reached_accuracy(self, logs):
        return (logs.get('accuracy') > 0.92  and  logs.get('val_accuracy') > 0.92)

callbacks = stopCallBack()

history = model.fit(train_generator,
              epochs=32,
              steps_per_epoch = 19,
              validation_data=validation_generator,
              callbacks = [callbacks, reduce_lr],
              )

"""# Plot Visualization"""

plt.figure(figsize=(12,4))

plt.subplot(121)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""# Predict Image

"""

import numpy as np
from tensorflow.keras.preprocessing import image
import matplotlib.image as mpimg

train_generator.class_indices
uploaded = files.upload()

for fn in uploaded.keys():
    path = fn
    img = image.load_img(path, target_size=(224, 224 ))

    imgplot = plt.imshow(img)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    images = np.vstack([x])

    classes = model.predict(images, batch_size=10)
    output_class = np.argmax(classes)
    print(fn)
    print(classes[0])
    print(output_class)

train_generator.class_indices
print(train_generator.class_indices)

if output_class == 0:
    print('Arborio')
elif output_class == 1:
    print('Basmati')
elif output_class == 2:
    print('Ipsala')
elif output_class == 3:
    print('Jasmine')
else:
    print('Karacadag')

tflite_model_name = "best_model.tflite"
converter = tf.lite.TFLiteConverter.from_keras_model(model)
save_model_format = converter.convert()

with open(tflite_model_name, 'wb') as f:
    f.write(save_model_format)